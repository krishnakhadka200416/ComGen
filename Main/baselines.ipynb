{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on travelcustomer/tvae_travelcustomer_train_42\n",
      "Save csv on travelcustomer/tvae_travelcustomer_train_42\n",
      "working on travelcustomer/copula_gan_travelcustomer_train_42\n",
      "Save csv on travelcustomer/copula_gan_travelcustomer_train_42\n",
      "working on travelcustomer/ctgan_travelcustomer_train_42\n",
      "Save csv on travelcustomer/ctgan_travelcustomer_train_42\n",
      "working on travelcustomer/tvae_travelcustomer_train_52\n",
      "Save csv on travelcustomer/tvae_travelcustomer_train_52\n",
      "working on travelcustomer/copula_gan_travelcustomer_train_52\n",
      "Save csv on travelcustomer/copula_gan_travelcustomer_train_52\n",
      "working on travelcustomer/ctgan_travelcustomer_train_52\n",
      "Save csv on travelcustomer/ctgan_travelcustomer_train_52\n",
      "working on travelcustomer/tvae_travelcustomer_train_62\n",
      "Save csv on travelcustomer/tvae_travelcustomer_train_62\n",
      "working on travelcustomer/copula_gan_travelcustomer_train_62\n",
      "Save csv on travelcustomer/copula_gan_travelcustomer_train_62\n",
      "working on travelcustomer/ctgan_travelcustomer_train_62\n",
      "Save csv on travelcustomer/ctgan_travelcustomer_train_62\n",
      "working on travelcustomer/tvae_travelcustomer_train_72\n",
      "Save csv on travelcustomer/tvae_travelcustomer_train_72\n",
      "working on travelcustomer/copula_gan_travelcustomer_train_72\n",
      "Save csv on travelcustomer/copula_gan_travelcustomer_train_72\n",
      "working on travelcustomer/ctgan_travelcustomer_train_72\n",
      "Save csv on travelcustomer/ctgan_travelcustomer_train_72\n",
      "working on adult/tvae_adult_train_42\n",
      "Save csv on adult/tvae_adult_train_42\n",
      "working on adult/copula_gan_adult_train_42\n",
      "Save csv on adult/copula_gan_adult_train_42\n",
      "working on adult/ctgan_adult_train_42\n",
      "Save csv on adult/ctgan_adult_train_42\n",
      "working on adult/tvae_adult_train_52\n",
      "Save csv on adult/tvae_adult_train_52\n",
      "working on adult/copula_gan_adult_train_52\n",
      "Save csv on adult/copula_gan_adult_train_52\n",
      "working on adult/ctgan_adult_train_52\n",
      "Save csv on adult/ctgan_adult_train_52\n",
      "working on adult/tvae_adult_train_62\n",
      "Save csv on adult/tvae_adult_train_62\n",
      "working on adult/copula_gan_adult_train_62\n",
      "Save csv on adult/copula_gan_adult_train_62\n",
      "working on adult/ctgan_adult_train_62\n",
      "Save csv on adult/ctgan_adult_train_62\n",
      "working on adult/tvae_adult_train_72\n",
      "Save csv on adult/tvae_adult_train_72\n",
      "working on adult/copula_gan_adult_train_72\n",
      "Save csv on adult/copula_gan_adult_train_72\n",
      "working on adult/ctgan_adult_train_72\n",
      "Save csv on adult/ctgan_adult_train_72\n",
      "working on creditdefault/tvae_creditdefault_train_42\n",
      "Save csv on creditdefault/tvae_creditdefault_train_42\n",
      "working on creditdefault/copula_gan_creditdefault_train_42\n",
      "Save csv on creditdefault/copula_gan_creditdefault_train_42\n",
      "working on creditdefault/ctgan_creditdefault_train_42\n",
      "Save csv on creditdefault/ctgan_creditdefault_train_42\n",
      "working on creditdefault/tvae_creditdefault_train_52\n",
      "Save csv on creditdefault/tvae_creditdefault_train_52\n",
      "working on creditdefault/copula_gan_creditdefault_train_52\n",
      "Save csv on creditdefault/copula_gan_creditdefault_train_52\n",
      "working on creditdefault/ctgan_creditdefault_train_52\n",
      "Save csv on creditdefault/ctgan_creditdefault_train_52\n",
      "working on creditdefault/tvae_creditdefault_train_62\n",
      "Save csv on creditdefault/tvae_creditdefault_train_62\n",
      "working on creditdefault/copula_gan_creditdefault_train_62\n",
      "Save csv on creditdefault/copula_gan_creditdefault_train_62\n",
      "working on creditdefault/ctgan_creditdefault_train_62\n",
      "Save csv on creditdefault/ctgan_creditdefault_train_62\n",
      "working on creditdefault/tvae_creditdefault_train_72\n",
      "Save csv on creditdefault/tvae_creditdefault_train_72\n",
      "working on creditdefault/copula_gan_creditdefault_train_72\n",
      "Save csv on creditdefault/copula_gan_creditdefault_train_72\n",
      "working on creditdefault/ctgan_creditdefault_train_72\n",
      "Save csv on creditdefault/ctgan_creditdefault_train_72\n",
      "working on heloc/tvae_heloc_train_42\n",
      "Save csv on heloc/tvae_heloc_train_42\n",
      "working on heloc/copula_gan_heloc_train_42\n",
      "Save csv on heloc/copula_gan_heloc_train_42\n",
      "working on heloc/ctgan_heloc_train_42\n",
      "Save csv on heloc/ctgan_heloc_train_42\n",
      "working on heloc/tvae_heloc_train_52\n",
      "Save csv on heloc/tvae_heloc_train_52\n",
      "working on heloc/copula_gan_heloc_train_52\n",
      "Save csv on heloc/copula_gan_heloc_train_52\n",
      "working on heloc/ctgan_heloc_train_52\n",
      "Save csv on heloc/ctgan_heloc_train_52\n",
      "working on heloc/tvae_heloc_train_62\n",
      "Save csv on heloc/tvae_heloc_train_62\n",
      "working on heloc/copula_gan_heloc_train_62\n",
      "Save csv on heloc/copula_gan_heloc_train_62\n",
      "working on heloc/ctgan_heloc_train_62\n",
      "Save csv on heloc/ctgan_heloc_train_62\n",
      "working on heloc/tvae_heloc_train_72\n",
      "Save csv on heloc/tvae_heloc_train_72\n",
      "working on heloc/copula_gan_heloc_train_72\n",
      "Save csv on heloc/copula_gan_heloc_train_72\n",
      "working on heloc/ctgan_heloc_train_72\n",
      "Save csv on heloc/ctgan_heloc_train_72\n"
     ]
    }
   ],
   "source": [
    "# Baselines \n",
    "# Use sdv_new \n",
    "#Save the models\n",
    "from sdv.single_table import TVAESynthesizer, GaussianCopulaSynthesizer, CTGANSynthesizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizers = [TVAESynthesizer, GaussianCopulaSynthesizer, CTGANSynthesizer]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42,52,62,72]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for random_state in random_states:\n",
    "        filepath = f'./{dataset}/original_{dataset}_train_{random_state}.csv'\n",
    "        train_df = pd.read_csv(filepath)\n",
    "        metadata = SingleTableMetadata()\n",
    "        metadata.detect_from_dataframe(data=train_df)\n",
    "        i = 0\n",
    "        for synthesizer in synthesizers:\n",
    "            print(f'working on {dataset}/{synthesizer_names[i]}_{dataset}_train_{random_state}')\n",
    "            s = synthesizer(metadata)\n",
    "            s.fit(train_df)\n",
    "            synth_samples_size = train_df.shape[0]\n",
    "            synthetic_samples = s.sample(synth_samples_size)\n",
    "            synthetic_samples.to_csv(f'./{dataset}/{synthesizer_names[i]}_{dataset}_train_{random_state}.csv', index=False)\n",
    "            print(f'Save csv on {dataset}/{synthesizer_names[i]}_{dataset}_train_{random_state}')\n",
    "            i +=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_new/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Dataset Synthesizer  Random_State  \\\n",
      "0   travelcustomer       ctvae            42   \n",
      "1   travelcustomer       ctvae            42   \n",
      "2   travelcustomer       ctvae            42   \n",
      "3   travelcustomer       ctvae            42   \n",
      "4            adult       ctvae            42   \n",
      "5            adult       ctvae            42   \n",
      "6            adult       ctvae            42   \n",
      "7            adult       ctvae            42   \n",
      "8    creditdefault       ctvae            42   \n",
      "9    creditdefault       ctvae            42   \n",
      "10   creditdefault       ctvae            42   \n",
      "11   creditdefault       ctvae            42   \n",
      "12           heloc       ctvae            42   \n",
      "13           heloc       ctvae            42   \n",
      "14           heloc       ctvae            42   \n",
      "15           heloc       ctvae            42   \n",
      "\n",
      "                           Classifier  Accuracy   ROC_AUC    PR_AUC  \n",
      "0   LogisticRegression(max_iter=1000)  0.773519  0.706621  0.481236  \n",
      "1            DecisionTreeClassifier()  0.602787  0.597771  0.501628  \n",
      "2            RandomForestClassifier()  0.644599  0.744192  0.435213  \n",
      "3               SVC(probability=True)  0.763066  0.717029  0.532247  \n",
      "4   LogisticRegression(max_iter=1000)  0.604245  0.655166  0.464987  \n",
      "5            DecisionTreeClassifier()  0.572920  0.585693  0.499137  \n",
      "6            RandomForestClassifier()  0.708114  0.695435  0.420445  \n",
      "7               SVC(probability=True)  0.627721  0.542091  0.312693  \n",
      "8   LogisticRegression(max_iter=1000)  0.693333  0.656034  0.321967  \n",
      "9            DecisionTreeClassifier()  0.696667  0.598098  0.445294  \n",
      "10           RandomForestClassifier()  0.799444  0.727797  0.465861  \n",
      "11              SVC(probability=True)  0.677000  0.628420  0.299996  \n",
      "12  LogisticRegression(max_iter=1000)  0.688018  0.732434  0.705813  \n",
      "13           DecisionTreeClassifier()  0.644359  0.642651  0.712013  \n",
      "14           RandomForestClassifier()  0.681644  0.753422  0.731375  \n",
      "15              SVC(probability=True)  0.694391  0.730985  0.708339  \n"
     ]
    }
   ],
   "source": [
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "# synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "# synthesizer_names = [\"original\",]\n",
    "synthesizer_names = [\"ctvae\",]\n",
    "# random_states = [42, 52, 62, 72]\n",
    "random_states = [42]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for synthesizer_name in synthesizer_names:\n",
    "        for random_state in random_states:\n",
    "            # Read the training and test data\n",
    "            # train_df = pd.read_csv(f'./{dataset}/{synthesizer_name}_{dataset}_train_{random_state}.csv')\n",
    "\n",
    "            train_df = pd.read_csv(f'./{dataset}/{synthesizer_name}_{dataset}_synth_{random_state}_False_4_way.csv')\n",
    "            test_df = pd.read_csv(f'./{dataset}/original_{dataset}_test_{random_state}.csv')\n",
    "\n",
    "            # Splitting features and labels\n",
    "            ht = HyperTransformer()\n",
    "            X_train = train_df.iloc[:, :-1]\n",
    "            ht.detect_initial_config(data=X_train)\n",
    "            X_train = ht.fit_transform(X_train)\n",
    "            y_train = train_df.iloc[:, -1]\n",
    "\n",
    "            X_test = test_df.iloc[:, :-1]\n",
    "            ht.detect_initial_config(data=X_test)\n",
    "            X_test = ht.fit_transform(X_test)\n",
    "            y_test = test_df.iloc[:, -1]\n",
    "\n",
    "            # Initialize classifiers\n",
    "            lr = LogisticRegression(max_iter=1000)\n",
    "            dt = DecisionTreeClassifier()\n",
    "            rf = RandomForestClassifier()\n",
    "            svc = SVC(probability=True)  # Set probability to True for SVC\n",
    "\n",
    "            classifiers = [lr, dt, rf, svc]\n",
    "\n",
    "            for classifier in classifiers:\n",
    "                # Fit and predict using the classifier\n",
    "                classifier.fit(X_train, y_train)\n",
    "                y_pred = classifier.predict(X_test)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Calculate ROC AUC\n",
    "                if hasattr(classifier, \"predict_proba\"):\n",
    "                    roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "                else:\n",
    "                    roc_auc = None\n",
    "\n",
    "                # Calculate Precision-Recall AUC\n",
    "                if hasattr(classifier, \"predict_proba\"):\n",
    "                    precision, recall, _ = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                else:\n",
    "                    pr_auc = None\n",
    "\n",
    "                # Add results to the array\n",
    "                results.append([dataset, synthesizer_name, random_state, str(classifier), accuracy, roc_auc, pr_auc])\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "print(results_df)\n",
    "# Export results as a CSV file\n",
    "# results_df.to_csv(\"original_tabular.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy for Different Random States:\n",
      "Dataset         Synthesizer  Classifier                       \n",
      "adult           original     DecisionTreeClassifier()             0.747373\n",
      "                             LogisticRegression(max_iter=1000)    0.800297\n",
      "                             RandomForestClassifier()             0.821709\n",
      "                             SVC(probability=True)                0.800604\n",
      "creditdefault   original     DecisionTreeClassifier()             0.725583\n",
      "                             LogisticRegression(max_iter=1000)    0.781250\n",
      "                             RandomForestClassifier()             0.816167\n",
      "                             SVC(probability=True)                0.781250\n",
      "heloc           original     DecisionTreeClassifier()             0.632409\n",
      "                             LogisticRegression(max_iter=1000)    0.717097\n",
      "                             RandomForestClassifier()             0.719168\n",
      "                             SVC(probability=True)                0.716539\n",
      "travelcustomer  original     DecisionTreeClassifier()             0.734321\n",
      "                             LogisticRegression(max_iter=1000)    0.747387\n",
      "                             RandomForestClassifier()             0.759582\n",
      "                             SVC(probability=True)                0.762195\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the results from the CSV file\n",
    "# results_df = pd.read_csv(\"tabular_results.csv\")\n",
    "#Original Data Average\n",
    "results_df = pd.read_csv(\"original_tabular.csv\")\n",
    "# Group by Random_State and compute the average accuracy\n",
    "average_accuracies = results_df.groupby([\"Dataset\",\"Synthesizer\", \"Classifier\"])[\"Accuracy\"].mean()\n",
    "\n",
    "print(\"Average Accuracy for Different Random States:\")\n",
    "print(average_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travelcustomer_tvae_42: KLD: 0.7512895213238691,  KS_test: 0.7636181909045477\n",
      "travelcustomer_tvae_52: KLD: 0.7000539963832053,  KS_test: 0.6956521739130435\n",
      "travelcustomer_tvae_62: KLD: 0.6614089802809908,  KS_test: 0.6881559220389805\n",
      "travelcustomer_tvae_72: KLD: 0.6590372887928703,  KS_test: 0.6786606696651672\n",
      "travelcustomer_copula_gan_42: KLD: 0.6374969301632873,  KS_test: 0.824087956021989\n",
      "travelcustomer_copula_gan_52: KLD: 0.6814920012308561,  KS_test: 0.8500749625187406\n",
      "travelcustomer_copula_gan_62: KLD: 0.6586030462994104,  KS_test: 0.8160919540229884\n",
      "travelcustomer_copula_gan_72: KLD: 0.6496891875980255,  KS_test: 0.8105947026486757\n",
      "travelcustomer_ctgan_42: KLD: 0.7691260301982077,  KS_test: 0.896551724137931\n",
      "travelcustomer_ctgan_52: KLD: 0.7410068169137854,  KS_test: 0.7981009495252374\n",
      "travelcustomer_ctgan_62: KLD: 0.6930600968060188,  KS_test: 0.8405797101449276\n",
      "travelcustomer_ctgan_72: KLD: 0.65215685161038,  KS_test: 0.8045977011494253\n",
      "adult_tvae_42: KLD: 0.9255663120991164,  KS_test: 0.8996377280913244\n",
      "adult_tvae_52: KLD: 0.9122230539878371,  KS_test: 0.8639913422445815\n",
      "adult_tvae_62: KLD: 0.9104766960027713,  KS_test: 0.8648980666296178\n",
      "adult_tvae_72: KLD: 0.8785796292320838,  KS_test: 0.8287628017365652\n",
      "adult_copula_gan_42: KLD: 0.7977967865651922,  KS_test: 0.8135114468730544\n",
      "adult_copula_gan_52: KLD: 0.7762476097647117,  KS_test: 0.8377840825996666\n",
      "adult_copula_gan_62: KLD: 0.7525381964536182,  KS_test: 0.6978309648466717\n",
      "adult_copula_gan_72: KLD: 0.7381852903147889,  KS_test: 0.759028593156529\n",
      "adult_ctgan_42: KLD: 0.8849287711499783,  KS_test: 0.8748762133184024\n",
      "adult_ctgan_52: KLD: 0.8711862900160554,  KS_test: 0.7962418990234953\n",
      "adult_ctgan_62: KLD: 0.8903373304570809,  KS_test: 0.8683912536613698\n",
      "adult_ctgan_72: KLD: 0.8893505203970162,  KS_test: 0.8233475261466721\n",
      "creditdefault_tvae_42: KLD: 0.9354810281245632,  KS_test: 0.9088452380952381\n",
      "creditdefault_tvae_52: KLD: 0.9018257494262887,  KS_test: 0.8742857142857142\n",
      "creditdefault_tvae_62: KLD: 0.9278176028546071,  KS_test: 0.8988571428571429\n",
      "creditdefault_tvae_72: KLD: 0.9342072521601714,  KS_test: 0.904142857142857\n",
      "creditdefault_copula_gan_42: KLD: 0.6421003477261473,  KS_test: 0.8309246031746031\n",
      "creditdefault_copula_gan_52: KLD: 0.5873495115333749,  KS_test: 0.824248015873016\n",
      "creditdefault_copula_gan_62: KLD: 0.5619117930802825,  KS_test: 0.8123412698412699\n",
      "creditdefault_copula_gan_72: KLD: 0.5869327308049109,  KS_test: 0.8277698412698413\n",
      "creditdefault_ctgan_42: KLD: 0.9634234148013477,  KS_test: 0.919188492063492\n",
      "creditdefault_ctgan_52: KLD: 0.9627843727161361,  KS_test: 0.9265912698412698\n",
      "creditdefault_ctgan_62: KLD: 0.959289119005001,  KS_test: 0.9165555555555556\n",
      "creditdefault_ctgan_72: KLD: 0.9584532154703153,  KS_test: 0.9153253968253967\n",
      "heloc_tvae_42: KLD: 0.8658116725009157,  KS_test: 0.9010096525975504\n",
      "heloc_tvae_52: KLD: 0.8525432627525487,  KS_test: 0.897970450302782\n",
      "heloc_tvae_62: KLD: 0.8567149754056131,  KS_test: 0.9051188362245596\n",
      "heloc_tvae_72: KLD: 0.878423349348326,  KS_test: 0.91086714019032\n",
      "heloc_copula_gan_42: KLD: 0.3186259582006056,  KS_test: 0.7690775394982472\n",
      "heloc_copula_gan_52: KLD: 0.35679928558962276,  KS_test: 0.7349519646678505\n",
      "heloc_copula_gan_62: KLD: 0.3112278020670175,  KS_test: 0.7488503392068478\n",
      "heloc_copula_gan_72: KLD: 0.33619865559872897,  KS_test: 0.7839149933979875\n",
      "heloc_ctgan_42: KLD: 0.8733957006207729,  KS_test: 0.9268940946136685\n",
      "heloc_ctgan_52: KLD: 0.8736832287034866,  KS_test: 0.9263761781177435\n",
      "heloc_ctgan_62: KLD: 0.877755273772973,  KS_test: 0.9307870054182033\n",
      "heloc_ctgan_72: KLD: 0.8683079593361123,  KS_test: 0.9185675909484132\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate with Statistical Evaluation\n",
    "\n",
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "from sdv.metrics.tabular import ContinuousKLDivergence, DiscreteKLDivergence,  KSComplement\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42, 52, 62, 72]\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for synthesizer_name in synthesizer_names:\n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/{synthesizer_name}_{dataset}_train_{random_state}.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "\n",
    "            cont_KLD = ContinuousKLDivergence.compute(original_df,synth_df)\n",
    "            try:\n",
    "                cat_KLD = DiscreteKLDivergence.compute(original_df, synth_df)\n",
    "            except Exception as e:\n",
    "                cat_KLD = cont_KLD\n",
    "            KLD = (cat_KLD + cont_KLD)/2\n",
    "\n",
    "            ks_test = KSComplement.compute(original_df, synth_df)\n",
    "            print(f'{dataset}_{synthesizer_name}_{random_state}: KLD: {KLD},  KS_test: {ks_test}')\n",
    "            \n",
    "\n",
    "        \n",
    "# Convert results to a DataFrame\n",
    "# results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# # Export results as a CSV file\n",
    "# results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average for travelcustomer_True: KLD: 0.813742498401042, KS_test: 0.8831637326860249\n",
      "Average for travelcustomer_False: KLD: 0.8527883008816997, KS_test: 0.8866855841293968\n",
      "Average for adult_True: KLD: 0.8589174029181508, KS_test: 0.7099127739852209\n",
      "Average for adult_False: KLD: 0.8377275144177441, KS_test: 0.771715887210191\n",
      "Average for creditdefault_True: KLD: 0.8169601369112383, KS_test: 0.8049167251708322\n",
      "Average for creditdefault_False: KLD: 0.8395375985164992, KS_test: 0.8961905680592541\n",
      "Average for heloc_True: KLD: 0.829303712890199, KS_test: 0.8581186768822671\n",
      "Average for heloc_False: KLD: 0.9096828363301184, KS_test: 0.9183692987334953\n"
     ]
    }
   ],
   "source": [
    "#Returns the average KLD and KS Test\n",
    "# This is for CT_VAE\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42, 52, 62, 72]\n",
    "dps = [True, False]\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        total_KLD = 0\n",
    "        total_KS_test = 0\n",
    "        num_random_states = len(random_states)\n",
    "        \n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/ctvae_{dataset}_synth_42_{dp}_4_way.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "\n",
    "            cont_KLD = ContinuousKLDivergence.compute(original_df, synth_df)\n",
    "            try:\n",
    "                cat_KLD = DiscreteKLDivergence.compute(original_df, synth_df)\n",
    "            except Exception as e:\n",
    "                cat_KLD = cont_KLD\n",
    "            KLD = (cat_KLD + cont_KLD) / 2\n",
    "\n",
    "            ks_test = KSComplement.compute(original_df, synth_df)\n",
    "            \n",
    "            total_KLD += KLD\n",
    "            total_KS_test += ks_test\n",
    "\n",
    "        avg_KLD = total_KLD / num_random_states\n",
    "        avg_KS_test = total_KS_test / num_random_states\n",
    "        \n",
    "        print(f'Average for {dataset}_{dp}: KLD: {avg_KLD }, KS_test: {avg_KS_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average for travelcustomer_True: KLD: 0.635456788373729, KS_test: 0.6893078690012792\n",
      "Average for adult_True: KLD: 0.8180808629885759, KS_test: 0.5592157585698175\n",
      "Average for creditdefault_True: KLD: 0.7335236083448036, KS_test: 0.7404680537352556\n",
      "Average for heloc_True: KLD: 0.712894349938328, KS_test: 0.7311110931592385\n"
     ]
    }
   ],
   "source": [
    "#Returns the average KLD and KS Test\n",
    "# This is for DP-TVE\n",
    "\n",
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "from sdv.metrics.tabular import ContinuousKLDivergence, DiscreteKLDivergence,  KSComplement\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42, 52, 62, 72]\n",
    "dps = [True]\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        total_KLD = 0\n",
    "        total_KS_test = 0\n",
    "        num_random_states = len(random_states)\n",
    "        \n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/dptvae_{dataset}_synth_42_{dp}_4_way.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "\n",
    "            cont_KLD = ContinuousKLDivergence.compute(original_df, synth_df)\n",
    "            try:\n",
    "                cat_KLD = DiscreteKLDivergence.compute(original_df, synth_df)\n",
    "            except Exception as e:\n",
    "                cat_KLD = cont_KLD\n",
    "            KLD = (cat_KLD + cont_KLD) / 2\n",
    "\n",
    "            ks_test = KSComplement.compute(original_df, synth_df)\n",
    "            \n",
    "            total_KLD += KLD\n",
    "            total_KS_test += ks_test\n",
    "\n",
    "        avg_KLD = total_KLD / num_random_states\n",
    "        avg_KS_test = total_KS_test / num_random_states\n",
    "        \n",
    "        print(f'Average for {dataset}_{dp}: KLD: {avg_KLD }, KS_test: {avg_KS_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travelcustomer_tvae_42: Numerical Privacy: 0.06891647738049293, Categorical Privacy: 0.23838080959520236, Privacy: 0.15364864348784765\n",
      "travelcustomer_copula_gan_42: Numerical Privacy: 0.09554602875970127, Categorical Privacy: 0.26086956521739135, Privacy: 0.1782077969885463\n",
      "travelcustomer_ctgan_42: Numerical Privacy: 0.0696759617287764, Categorical Privacy: 0.2278860569715142, Privacy: 0.1487810093501453\n",
      "adult_tvae_42: Numerical Privacy: 0.2882446797377596, Categorical Privacy: 0.25341484103074086, Privacy: 0.2708297603842502\n",
      "adult_copula_gan_42: Numerical Privacy: 0.2958557026473682, Categorical Privacy: 0.27558571470355964, Privacy: 0.28572070867546395\n",
      "adult_ctgan_42: Numerical Privacy: 0.26388660622896, Categorical Privacy: 0.2194565503524526, Privacy: 0.2416715782907063\n",
      "creditdefault_tvae_42: Numerical Privacy: 0.27161795990174176, Categorical Privacy: 0.27161795990174176, Privacy: 0.27161795990174176\n",
      "creditdefault_copula_gan_42: Numerical Privacy: 0.29038983181339667, Categorical Privacy: 0.29038983181339667, Privacy: 0.29038983181339667\n",
      "creditdefault_ctgan_42: Numerical Privacy: 0.2849706176674125, Categorical Privacy: 0.2849706176674125, Privacy: 0.2849706176674125\n",
      "heloc_tvae_42: Numerical Privacy: 0.08941163992511283, Categorical Privacy: 0.08941163992511283, Privacy: 0.08941163992511283\n",
      "heloc_copula_gan_42: Numerical Privacy: 0.11795882929120471, Categorical Privacy: 0.11795882929120471, Privacy: 0.11795882929120471\n",
      "heloc_ctgan_42: Numerical Privacy: 0.07910743696470422, Categorical Privacy: 0.07910743696470422, Privacy: 0.07910743696470422\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate with Statistical Evaluation\n",
    "#For Baselines\n",
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "from sdv.metrics.tabular import NumericalPrivacyMetric, CategoricalPrivacyMetric\n",
    "from sdmetrics.single_table import CategoricalKNN, NumericalMLP\n",
    "import warnings\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "warnings.filterwarnings('ignore')\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42]\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for synthesizer_name in synthesizer_names:\n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/{synthesizer_name}_{dataset}_train_{random_state}.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(data=original_df)\n",
    "            categorical_keys = []\n",
    "            continuous_keys = []\n",
    "            target_key = []\n",
    "            columns_metadata = metadata.columns\n",
    "            last_key = list(columns_metadata.keys())[-1]\n",
    "            target_key.append(last_key)\n",
    "\n",
    "            for key, value in columns_metadata.items():\n",
    "                if key == last_key:\n",
    "                    continue  # Skip the last key since it's already in the \"other\" category\n",
    "                if value[\"sdtype\"] == \"categorical\":\n",
    "                    categorical_keys.append(key)\n",
    "                elif value[\"sdtype\"] == \"numerical\":\n",
    "                    continuous_keys.append(key)\n",
    "            # print(categorical_keys, continuous_keys, target_key)\n",
    "            # continue\n",
    "            num_privacy = NumericalMLP.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=continuous_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            try:\n",
    "                cat_privacy = CategoricalKNN.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=categorical_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            except Exception as e:\n",
    "                cat_privacy = num_privacy\n",
    "            privacy_score = (num_privacy + cat_privacy)/2\n",
    "\n",
    "\n",
    "            print(f'{dataset}_{synthesizer_name}_{random_state}: Numerical Privacy: {num_privacy}, Categorical Privacy: {cat_privacy}, Privacy: {privacy_score}')\n",
    "            \n",
    "\n",
    "        \n",
    "# Convert results to a DataFrame\n",
    "# results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# # Export results as a CSV file\n",
    "# results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travelcustomer_True_42: Numerical Privacy: 0.1490787091588749, Categorical Privacy: 0.26686656671664166, Privacy: 0.20797263793775828\n",
      "travelcustomer_False_42: Numerical Privacy: 0.09247190185164218, Categorical Privacy: 0.23838080959520236, Privacy: 0.16542635572342226\n",
      "adult_True_42: Numerical Privacy: 0.23108616972739043, Categorical Privacy: 0.6998449793793325, Privacy: 0.46546557455336146\n",
      "adult_False_42: Numerical Privacy: 0.2313228472715006, Categorical Privacy: 0.4195501477083272, Privacy: 0.3254364974899139\n",
      "creditdefault_True_42: Numerical Privacy: 0.30063922252040404, Categorical Privacy: 0.30063922252040404, Privacy: 0.30063922252040404\n",
      "creditdefault_False_42: Numerical Privacy: 0.29699793995894735, Categorical Privacy: 0.29699793995894735, Privacy: 0.29699793995894735\n",
      "heloc_True_42: Numerical Privacy: 0.17292430094296862, Categorical Privacy: 0.17292430094296862, Privacy: 0.17292430094296862\n",
      "heloc_False_42: Numerical Privacy: 0.12696824319191005, Categorical Privacy: 0.12696824319191005, Privacy: 0.12696824319191005\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate with Statistical Evaluation\n",
    "#For CT-VAE with DP and non DP\n",
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "from sdv.metrics.tabular import NumericalPrivacyMetric, CategoricalPrivacyMetric\n",
    "from sdmetrics.single_table import CategoricalKNN, NumericalMLP\n",
    "import warnings\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "warnings.filterwarnings('ignore')\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42]\n",
    "results = []\n",
    "dps = [True, False]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/ctvae_{dataset}_synth_42_{dp}_4_way.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(data=original_df)\n",
    "            categorical_keys = []\n",
    "            continuous_keys = []\n",
    "            target_key = []\n",
    "            columns_metadata = metadata.columns\n",
    "            last_key = list(columns_metadata.keys())[-1]\n",
    "            target_key.append(last_key)\n",
    "\n",
    "            for key, value in columns_metadata.items():\n",
    "                if key == last_key:\n",
    "                    continue  # Skip the last key since it's already in the \"other\" category\n",
    "                if value[\"sdtype\"] == \"categorical\":\n",
    "                    categorical_keys.append(key)\n",
    "                elif value[\"sdtype\"] == \"numerical\":\n",
    "                    continuous_keys.append(key)\n",
    "            # print(categorical_keys, continuous_keys, target_key)\n",
    "            # continue\n",
    "            num_privacy = NumericalMLP.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=continuous_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            try:\n",
    "                cat_privacy = CategoricalKNN.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=categorical_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            except Exception as e:\n",
    "                cat_privacy = num_privacy\n",
    "            privacy_score = (num_privacy + cat_privacy)/2\n",
    "\n",
    "\n",
    "            print(f'{dataset}_{dp}_{random_state}: Numerical Privacy: {num_privacy}, Categorical Privacy: {cat_privacy}, Privacy: {privacy_score}')\n",
    "            \n",
    "\n",
    "        \n",
    "# Convert results to a DataFrame\n",
    "# results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# # Export results as a CSV file\n",
    "# results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travelcustomer_True_42: Numerical Privacy: 0.06849191236680591, Categorical Privacy: 0.24437781109445278, Privacy: 0.15643486173062934\n",
      "adult_True_42: Numerical Privacy: 0.3206637924281312, Categorical Privacy: 0.39530258270203866, Privacy: 0.35798318756508496\n",
      "creditdefault_True_42: Numerical Privacy: 0.28736415998678083, Categorical Privacy: 0.28736415998678083, Privacy: 0.28736415998678083\n",
      "heloc_True_42: Numerical Privacy: 0.20185080700781463, Categorical Privacy: 0.20185080700781463, Privacy: 0.20185080700781463\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluate with Statistical Evaluation\n",
    "#For DP-TVAE \n",
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "from sdv.metrics.tabular import NumericalPrivacyMetric, CategoricalPrivacyMetric\n",
    "from sdmetrics.single_table import CategoricalKNN, NumericalMLP\n",
    "import warnings\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "warnings.filterwarnings('ignore')\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42]\n",
    "results = []\n",
    "dps = [True]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/dptvae_{dataset}_synth_42_{dp}_4_way.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(data=original_df)\n",
    "            categorical_keys = []\n",
    "            continuous_keys = []\n",
    "            target_key = []\n",
    "            columns_metadata = metadata.columns\n",
    "            last_key = list(columns_metadata.keys())[-1]\n",
    "            target_key.append(last_key)\n",
    "\n",
    "            for key, value in columns_metadata.items():\n",
    "                if key == last_key:\n",
    "                    continue  # Skip the last key since it's already in the \"other\" category\n",
    "                if value[\"sdtype\"] == \"categorical\":\n",
    "                    categorical_keys.append(key)\n",
    "                elif value[\"sdtype\"] == \"numerical\":\n",
    "                    continuous_keys.append(key)\n",
    "            # print(categorical_keys, continuous_keys, target_key)\n",
    "            # continue\n",
    "            num_privacy = NumericalMLP.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=continuous_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            try:\n",
    "                cat_privacy = CategoricalKNN.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=categorical_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            except Exception as e:\n",
    "                cat_privacy = num_privacy\n",
    "            privacy_score = (num_privacy + cat_privacy)/2\n",
    "\n",
    "\n",
    "            print(f'{dataset}_{dp}_{random_state}: Numerical Privacy: {num_privacy}, Categorical Privacy: {cat_privacy}, Privacy: {privacy_score}')\n",
    "            \n",
    "\n",
    "        \n",
    "# Convert results to a DataFrame\n",
    "# results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# # Export results as a CSV file\n",
    "# results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
