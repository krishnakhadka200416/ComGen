{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IDGenerator' from 'rdt.transformers' (/Users/krishnakhadka/opt/anaconda3/envs/CT-VAE/lib/python3.7/site-packages/rdt/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mb/3281pz1x7d76n0l2lyz1yxvh0000gn/T/ipykernel_13204/4072345702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTVAESynthesizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleTableMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Research/ComGen/Main/sdv/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miter_entry_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from sdv import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_processing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     sampling, sequential, single_table)\n",
      "\u001b[0;32m~/Desktop/Research/ComGen/Main/sdv/data_processing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Data processing subpackage.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = (\n",
      "\u001b[0;32m~/Desktop/Research/ComGen/Main/sdv/data_processing/data_processor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_float_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnonymizedFaker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegexGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_default_transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IDGenerator' from 'rdt.transformers' (/Users/krishnakhadka/opt/anaconda3/envs/CT-VAE/lib/python3.7/site-packages/rdt/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sdv.single_table import TVAESynthesizer\n",
    "import pandas as pd\n",
    "from ctgan import CTGAN, TVAE\n",
    "import numpy as np\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "filepath = './data/credit_default.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_csv(filepath=filepath)\n",
    "latent_dim = 7\n",
    "\n",
    "synthesizer = TVAESynthesizer(metadata, \n",
    "                            embedding_dim=latent_dim, \n",
    "                            compress_dims=(128,64), \n",
    "                            decompress_dims=(64,128),\n",
    "                            epochs=300,\n",
    "                            dp = False,\n",
    "                            sigma = 0.4)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)  # 20% of the data will be used for testing\n",
    "\n",
    "#original Samples\n",
    "synthesizer.fit(train_df)\n",
    "synth_sample_size = train_df.shape[0]\n",
    "synth_samples = synthesizer.sample(synth_sample_size)\n",
    "synth_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing Latent Space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.775985</td>\n",
       "      <td>-0.571328</td>\n",
       "      <td>-0.568613</td>\n",
       "      <td>-0.557745</td>\n",
       "      <td>0.945785</td>\n",
       "      <td>0.958547</td>\n",
       "      <td>2.231675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.195817</td>\n",
       "      <td>-2.495292</td>\n",
       "      <td>-2.489547</td>\n",
       "      <td>-0.597041</td>\n",
       "      <td>0.988013</td>\n",
       "      <td>2.286619</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.805192</td>\n",
       "      <td>-1.484467</td>\n",
       "      <td>-0.864457</td>\n",
       "      <td>-0.421144</td>\n",
       "      <td>-0.308374</td>\n",
       "      <td>-0.035560</td>\n",
       "      <td>0.999729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.005682</td>\n",
       "      <td>2.834975</td>\n",
       "      <td>2.932684</td>\n",
       "      <td>3.052235</td>\n",
       "      <td>3.151231</td>\n",
       "      <td>3.306128</td>\n",
       "      <td>3.420756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.468690</td>\n",
       "      <td>-0.681231</td>\n",
       "      <td>-0.678634</td>\n",
       "      <td>-0.102560</td>\n",
       "      <td>0.709498</td>\n",
       "      <td>0.709936</td>\n",
       "      <td>1.571514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0 -1.775985 -0.571328 -0.568613 -0.557745  0.945785  0.958547  2.231675\n",
       "1 -3.195817 -2.495292 -2.489547 -0.597041  0.988013  2.286619       NaN\n",
       "2 -1.805192 -1.484467 -0.864457 -0.421144 -0.308374 -0.035560  0.999729\n",
       "3 -2.005682  2.834975  2.932684  3.052235  3.151231  3.306128  3.420756\n",
       "4 -2.468690 -0.681231 -0.678634 -0.102560  0.709498  0.709936  1.571514"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For t-way samples\n",
    "vae = synthesizer.get_vae()\n",
    "discretized_space = vae.get_discretized()\n",
    "discretized_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369061.055393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.568640</td>\n",
       "      <td>2.005300</td>\n",
       "      <td>2.004162</td>\n",
       "      <td>1.994447</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>...</td>\n",
       "      <td>368913.719194</td>\n",
       "      <td>299314.914384</td>\n",
       "      <td>261579.387798</td>\n",
       "      <td>17833.313386</td>\n",
       "      <td>11019.302479</td>\n",
       "      <td>14437.875723</td>\n",
       "      <td>16776.611381</td>\n",
       "      <td>11428.602129</td>\n",
       "      <td>14467.905921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367999.273828</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.555046</td>\n",
       "      <td>2.011514</td>\n",
       "      <td>2.004413</td>\n",
       "      <td>1.994509</td>\n",
       "      <td>2.006650</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>...</td>\n",
       "      <td>335090.511812</td>\n",
       "      <td>295882.434672</td>\n",
       "      <td>261990.132241</td>\n",
       "      <td>17639.872748</td>\n",
       "      <td>10215.656315</td>\n",
       "      <td>14685.438365</td>\n",
       "      <td>15902.018407</td>\n",
       "      <td>11341.589791</td>\n",
       "      <td>5916.243132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367982.802203</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.551517</td>\n",
       "      <td>2.011572</td>\n",
       "      <td>2.004400</td>\n",
       "      <td>1.994504</td>\n",
       "      <td>2.006644</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>...</td>\n",
       "      <td>334601.933715</td>\n",
       "      <td>295754.352129</td>\n",
       "      <td>261948.217799</td>\n",
       "      <td>17643.023157</td>\n",
       "      <td>10202.478854</td>\n",
       "      <td>14688.470348</td>\n",
       "      <td>15892.412464</td>\n",
       "      <td>11343.149658</td>\n",
       "      <td>5921.260420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84165.799955</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26.619928</td>\n",
       "      <td>2.009580</td>\n",
       "      <td>2.000407</td>\n",
       "      <td>2.004227</td>\n",
       "      <td>2.007354</td>\n",
       "      <td>2.000338</td>\n",
       "      <td>...</td>\n",
       "      <td>41829.386289</td>\n",
       "      <td>37371.890113</td>\n",
       "      <td>37018.791115</td>\n",
       "      <td>1081.291483</td>\n",
       "      <td>820.945857</td>\n",
       "      <td>422.266210</td>\n",
       "      <td>1740.478192</td>\n",
       "      <td>1454.726147</td>\n",
       "      <td>1762.259998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85393.863880</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26.501919</td>\n",
       "      <td>-1.000933</td>\n",
       "      <td>1.999151</td>\n",
       "      <td>-2.002576</td>\n",
       "      <td>-1.998578</td>\n",
       "      <td>-1.998176</td>\n",
       "      <td>...</td>\n",
       "      <td>434.178391</td>\n",
       "      <td>181.747767</td>\n",
       "      <td>100.056579</td>\n",
       "      <td>59.352068</td>\n",
       "      <td>-121.218643</td>\n",
       "      <td>21.777256</td>\n",
       "      <td>-22.591377</td>\n",
       "      <td>73.099794</td>\n",
       "      <td>-112.289106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>366313.140725</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36.338799</td>\n",
       "      <td>-0.000622</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>...</td>\n",
       "      <td>313210.787121</td>\n",
       "      <td>317938.560669</td>\n",
       "      <td>232511.407267</td>\n",
       "      <td>18057.373035</td>\n",
       "      <td>12271.389155</td>\n",
       "      <td>16367.344055</td>\n",
       "      <td>16294.352270</td>\n",
       "      <td>12095.233824</td>\n",
       "      <td>14673.823511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>362037.163483</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>36.205333</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>...</td>\n",
       "      <td>40958.246297</td>\n",
       "      <td>42160.187633</td>\n",
       "      <td>39694.553220</td>\n",
       "      <td>8755.033212</td>\n",
       "      <td>10292.975074</td>\n",
       "      <td>3495.142375</td>\n",
       "      <td>2126.217315</td>\n",
       "      <td>1598.148291</td>\n",
       "      <td>2208.636559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>365836.009534</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36.090208</td>\n",
       "      <td>-1.991773</td>\n",
       "      <td>-2.005225</td>\n",
       "      <td>-1.002076</td>\n",
       "      <td>-1.000480</td>\n",
       "      <td>-1.996775</td>\n",
       "      <td>...</td>\n",
       "      <td>12820.890844</td>\n",
       "      <td>365.534423</td>\n",
       "      <td>269.359150</td>\n",
       "      <td>7823.503552</td>\n",
       "      <td>653.732084</td>\n",
       "      <td>2704.992068</td>\n",
       "      <td>328.911368</td>\n",
       "      <td>1395.574912</td>\n",
       "      <td>166.114164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>358998.269042</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.162530</td>\n",
       "      <td>-2.001671</td>\n",
       "      <td>-1.997172</td>\n",
       "      <td>-2.002328</td>\n",
       "      <td>-1.997275</td>\n",
       "      <td>-1.992077</td>\n",
       "      <td>...</td>\n",
       "      <td>583.385500</td>\n",
       "      <td>368.810000</td>\n",
       "      <td>132.907670</td>\n",
       "      <td>750.337986</td>\n",
       "      <td>422.252969</td>\n",
       "      <td>133.565169</td>\n",
       "      <td>228.210155</td>\n",
       "      <td>55.777950</td>\n",
       "      <td>170.501206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>368135.324885</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40.855172</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001834</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>...</td>\n",
       "      <td>389721.079203</td>\n",
       "      <td>319923.490261</td>\n",
       "      <td>285390.361260</td>\n",
       "      <td>17035.459958</td>\n",
       "      <td>12502.060821</td>\n",
       "      <td>15577.055548</td>\n",
       "      <td>15612.779296</td>\n",
       "      <td>11689.463209</td>\n",
       "      <td>14216.915679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2401 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 X1  X2  X3  X4         X5        X6        X7        X8  \\\n",
       "0     369061.055393   1   1   2  29.568640  2.005300  2.004162  1.994447   \n",
       "1     367999.273828   1   1   2  29.555046  2.011514  2.004413  1.994509   \n",
       "2     367982.802203   1   1   2  29.551517  2.011572  2.004400  1.994504   \n",
       "3      84165.799955   2   1   2  26.619928  2.009580  2.000407  2.004227   \n",
       "4      85393.863880   2   1   2  26.501919 -1.000933  1.999151 -2.002576   \n",
       "...             ...  ..  ..  ..        ...       ...       ...       ...   \n",
       "2396  366313.140725   1   2   1  36.338799 -0.000622  0.000878  0.001023   \n",
       "2397  362037.163483   2   3   1  36.205333 -0.000977 -0.000546  0.000520   \n",
       "2398  365836.009534   2   2   1  36.090208 -1.991773 -2.005225 -1.002076   \n",
       "2399  358998.269042   2   1   1  36.162530 -2.001671 -1.997172 -2.002328   \n",
       "2400  368135.324885   1   2   1  40.855172  0.000798  0.000789  0.001029   \n",
       "\n",
       "            X9       X10  ...            X15            X16            X17  \\\n",
       "0     0.001904 -0.001230  ...  368913.719194  299314.914384  261579.387798   \n",
       "1     2.006650 -0.000639  ...  335090.511812  295882.434672  261990.132241   \n",
       "2     2.006644 -0.000627  ...  334601.933715  295754.352129  261948.217799   \n",
       "3     2.007354  2.000338  ...   41829.386289   37371.890113   37018.791115   \n",
       "4    -1.998578 -1.998176  ...     434.178391     181.747767     100.056579   \n",
       "...        ...       ...  ...            ...            ...            ...   \n",
       "2396  0.001542  0.002183  ...  313210.787121  317938.560669  232511.407267   \n",
       "2397 -0.000615  0.002807  ...   40958.246297   42160.187633   39694.553220   \n",
       "2398 -1.000480 -1.996775  ...   12820.890844     365.534423     269.359150   \n",
       "2399 -1.997275 -1.992077  ...     583.385500     368.810000     132.907670   \n",
       "2400  0.001834  0.000611  ...  389721.079203  319923.490261  285390.361260   \n",
       "\n",
       "               X18           X19           X20           X21           X22  \\\n",
       "0     17833.313386  11019.302479  14437.875723  16776.611381  11428.602129   \n",
       "1     17639.872748  10215.656315  14685.438365  15902.018407  11341.589791   \n",
       "2     17643.023157  10202.478854  14688.470348  15892.412464  11343.149658   \n",
       "3      1081.291483    820.945857    422.266210   1740.478192   1454.726147   \n",
       "4        59.352068   -121.218643     21.777256    -22.591377     73.099794   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2396  18057.373035  12271.389155  16367.344055  16294.352270  12095.233824   \n",
       "2397   8755.033212  10292.975074   3495.142375   2126.217315   1598.148291   \n",
       "2398   7823.503552    653.732084   2704.992068    328.911368   1395.574912   \n",
       "2399    750.337986    422.252969    133.565169    228.210155     55.777950   \n",
       "2400  17035.459958  12502.060821  15577.055548  15612.779296  11689.463209   \n",
       "\n",
       "               X23  Y  \n",
       "0     14467.905921  1  \n",
       "1      5916.243132  1  \n",
       "2      5921.260420  1  \n",
       "3      1762.259998  1  \n",
       "4      -112.289106  0  \n",
       "...            ... ..  \n",
       "2396  14673.823511  0  \n",
       "2397   2208.636559  0  \n",
       "2398    166.114164  0  \n",
       "2399    170.501206  0  \n",
       "2400  14216.915679  0  \n",
       "\n",
       "[2401 rows x 24 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_way_values = pd.read_csv('./DataCSV/discretized/credit_default/credit_default_5_4.csv')\n",
    "t_way_values\n",
    "synth_samples = vae.sample_t_way_latent(t_way_values)\n",
    "synth_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "(1/2) Evaluating Column Shapes: : 100%|██████████| 24/24 [00:00<00:00, 268.02it/s]\n",
      "(2/2) Evaluating Column Pair Trends: : 100%|██████████| 276/276 [00:05<00:00, 50.44it/s]\n",
      "\n",
      "Overall Quality Score: 58.74%\n",
      "\n",
      "Properties:\n",
      "- Column Shapes: 61.92%\n",
      "- Column Pair Trends: 55.57%\n"
     ]
    }
   ],
   "source": [
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "#Generate Synthetic Data\n",
    "\n",
    "quality_report = evaluate_quality(\n",
    "    real_data=train_df,\n",
    "    synthetic_data=synth_samples,\n",
    "    metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on train_df:\n",
      "Logistic Regression Accuracy: 0.8037670181185382\n",
      "Decision Tree Accuracy: 0.6742757702937865\n",
      "SVC Accuracy: 0.8035622888729655\n",
      "------------------------------------------------\n",
      "Trained on synth_samples:\n",
      "Logistic Regression Accuracy: 0.7828846350701197\n",
      "Decision Tree Accuracy: 0.7404033166137782\n",
      "SVC Accuracy: 0.7982393284880746\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "ht = HyperTransformer()\n",
    "# Assuming train_df, synth_samples, and test_df are already defined\n",
    "\n",
    "# Splitting features and labels\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "ht.detect_initial_config(data=X_train)\n",
    "\n",
    "X_train = ht.fit_transform(X_train)\n",
    "y_train = train_df.iloc[:, -1]\n",
    "\n",
    "X_synth = synth_samples.iloc[:, :-1]\n",
    "ht.detect_initial_config(data=X_synth)\n",
    "X_synth = ht.fit_transform(X_synth)\n",
    "y_synth = synth_samples.iloc[:, -1]\n",
    "\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "ht.detect_initial_config(data=X_test)\n",
    "X_test = ht.fit_transform(X_test)\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# Models initialization\n",
    "lr = LogisticRegression(max_iter=1000)  \n",
    "dt = DecisionTreeClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "# Train on train_df\n",
    "lr.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation on test_df after training on train_df\n",
    "print(\"Trained on train_df:\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr.predict(X_test)))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt.predict(X_test)))\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, svc.predict(X_test)))\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Train on synth_samples\n",
    "lr.fit(X_synth, y_synth)\n",
    "dt.fit(X_synth, y_synth)\n",
    "svc.fit(X_synth, y_synth)\n",
    "\n",
    "# Evaluation on test_df after training on synth_samples\n",
    "print(\"Trained on synth_samples:\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr.predict(X_test)))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt.predict(X_test)))\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, svc.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing Latent Space\n"
     ]
    }
   ],
   "source": [
    "vae = synthesizer.get_vae()\n",
    "\n",
    "discretized_Data = vae.get_discretized()\n",
    "\n",
    "discretized_df = pd.DataFrame(discretized_Data)\n",
    "discretized_df.to_csv(f'./DataCSV/discretized/CustomerTravel/discretized_l_{latent_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_way_latent_3 = pd.read_csv('DataCSV/discretized/CustomerTravel/TravelCustomer_4_3.csv')\n",
    "synthetic_sample = vae.sample_t_way_latent(t_way_latent_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>FrequentFlyer</th>\n",
       "      <th>AnnualIncomeClass</th>\n",
       "      <th>ServicesOpted</th>\n",
       "      <th>AccountSyncedToSocialMedia</th>\n",
       "      <th>BookedHotelOrNot</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.967147</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Low Income</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.836555</td>\n",
       "      <td>No</td>\n",
       "      <td>Low Income</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.106165</td>\n",
       "      <td>No</td>\n",
       "      <td>Middle Income</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.187775</td>\n",
       "      <td>No</td>\n",
       "      <td>Middle Income</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33.770327</td>\n",
       "      <td>No</td>\n",
       "      <td>Middle Income</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>35.068637</td>\n",
       "      <td>No</td>\n",
       "      <td>Middle Income</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>37.785111</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Low Income</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>35.472357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>High Income</td>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>29.754906</td>\n",
       "      <td>No</td>\n",
       "      <td>Low Income</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>30.034076</td>\n",
       "      <td>No</td>\n",
       "      <td>Middle Income</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age FrequentFlyer AnnualIncomeClass  ServicesOpted  \\\n",
       "0    26.967147           Yes        Low Income              5   \n",
       "1    26.836555            No        Low Income              5   \n",
       "2    27.106165            No     Middle Income              5   \n",
       "3    33.187775            No     Middle Income              1   \n",
       "4    33.770327            No     Middle Income              1   \n",
       "..         ...           ...               ...            ...   \n",
       "328  35.068637            No     Middle Income              1   \n",
       "329  37.785111           Yes        Low Income              2   \n",
       "330  35.472357           Yes       High Income              4   \n",
       "331  29.754906            No        Low Income              2   \n",
       "332  30.034076            No     Middle Income              2   \n",
       "\n",
       "    AccountSyncedToSocialMedia BookedHotelOrNot  Target  \n",
       "0                          Yes               No       1  \n",
       "1                          Yes              Yes       1  \n",
       "2                          Yes              Yes       0  \n",
       "3                          Yes              Yes       0  \n",
       "4                          Yes              Yes       0  \n",
       "..                         ...              ...     ...  \n",
       "328                        Yes              Yes       0  \n",
       "329                         No              Yes       0  \n",
       "330                         No               No       1  \n",
       "331                         No              Yes       0  \n",
       "332                         No               No       0  \n",
       "\n",
       "[333 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "(1/2) Evaluating Column Shapes: : 100%|██████████| 7/7 [00:00<00:00, 1530.37it/s]\n",
      "(2/2) Evaluating Column Pair Trends: : 100%|██████████| 21/21 [00:00<00:00, 122.55it/s]\n",
      "\n",
      "Overall Quality Score: 92.54%\n",
      "\n",
      "Properties:\n",
      "- Column Shapes: 96.6%\n",
      "- Column Pair Trends: 88.49%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on train_df:\n",
      "Logistic Regression Accuracy: 0.7811666666666667\n",
      "Decision Tree Accuracy: 0.7216666666666667\n",
      "Random Forest Accuracy: 0.8161666666666667\n",
      "SVC Accuracy: 0.7811666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on synth_samples:\n",
      "Logistic Regression Accuracy: 0.7316666666666667\n",
      "Decision Tree Accuracy: 0.7883333333333333\n",
      "Random Forest Accuracy: 0.7941666666666667\n",
      "SVC Accuracy: 0.738\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from rdt import HyperTransformer\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opacus import PrivacyEngine\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_nn(model, X_train_df, y_train_df, epochs=10, lr=0.001, batch_size=32):\n",
    "    # Convert data to tensors\n",
    "    X_train_tensor = df_to_tensor(X_train_df)\n",
    "    y_train_tensor = torch.tensor(y_train_df.values, dtype=torch.long)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    MAX_GRAD_NORM = 1.2\n",
    "    EPSILON = 50.0\n",
    "    DELTA = 1e-5\n",
    "    EPOCHS = 30\n",
    "    privacy_engine = PrivacyEngine()\n",
    "\n",
    "    model, optimizer, _ = privacy_engine.make_private_with_epsilon(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,  # Note that the DataLoader is passed here\n",
    "        epochs=EPOCHS,\n",
    "        target_epsilon=EPSILON,\n",
    "        target_delta=DELTA,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function for the neural network\n",
    "def evaluate_nn(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = outputs.max(1)\n",
    "        accuracy = (predicted == y_test).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "ht = HyperTransformer()\n",
    "\n",
    "# Assuming train_df, synth_samples, and test_df are already defined\n",
    "\n",
    "# Splitting features and labels\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "ht.detect_initial_config(data=X_train)\n",
    "X_train = ht.fit_transform(X_train)\n",
    "y_train = train_df.iloc[:, -1]\n",
    "\n",
    "X_synth = synth_samples.iloc[:, :-1]\n",
    "ht.detect_initial_config(data=X_synth)\n",
    "X_synth = ht.fit_transform(X_synth)\n",
    "y_synth = synth_samples.iloc[:, -1]\n",
    "\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "ht.detect_initial_config(data=X_test)\n",
    "X_test = ht.fit_transform(X_test)\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(TensorDataset(df_to_tensor(X_train), torch.tensor(y_train.values, dtype=torch.long)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "synth_loader = DataLoader(TensorDataset(df_to_tensor(X_synth), torch.tensor(y_synth.values, dtype=torch.long)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Models initialization\n",
    "lr = LogisticRegression(max_iter=1000)  \n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "\n",
    "# Train on train_df\n",
    "lr.fit(X_train, y_train)\n",
    "dt.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation on test_df after training on train_df\n",
    "print(\"Trained on train_df:\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr.predict(X_test)))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt.predict(X_test)))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf.predict(X_test)))\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, svc.predict(X_test)))\n",
    "\n",
    "\n",
    "lr1 = LogisticRegression(max_iter=1000)  \n",
    "dt1 = DecisionTreeClassifier()\n",
    "rf1 = RandomForestClassifier()\n",
    "svc1 = SVC()\n",
    "# Train on synth_samples\n",
    "lr1.fit(X_synth, y_synth)\n",
    "dt1.fit(X_synth, y_synth)\n",
    "rf1.fit(X_synth, y_synth)\n",
    "svc1.fit(X_synth, y_synth)\n",
    "# train_nn(nn_model, X_synth, y_synth)\n",
    "\n",
    "\n",
    "# Evaluation on test_df after training on synth_samples\n",
    "print(\"Trained on synth_samples:\")\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr1.predict(X_test)))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt1.predict(X_test)))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf1.predict(X_test)))\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, svc1.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get models for CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate and save models\n",
    "\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42, 52, 62, 72]\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for synthesizer_name in synthesizer_names:\n",
    "        for random_state in random_states:\n",
    "            train_df = pd.read_csv(f'./data/{dataset}/{synthesizer_name}_{dataset}_train_{random_state}.csv')          \n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(data=train_df)\n",
    "            latent_dim = 7\n",
    "            dps = [True, False]\n",
    "            for dp in dps:\n",
    "                print(f\"Working on {dataset} {synthesizer_name} {random_state} {dp}\")\n",
    "                synthesizer = TVAESynthesizer(metadata, \n",
    "                                            embedding_dim=latent_dim, \n",
    "                                            compress_dims=(128,64), \n",
    "                                            decompress_dims=(64,128),\n",
    "                                            epochs=300,\n",
    "                                            dp = dp,\n",
    "                                            sigma = 0.2)\n",
    "                synthesizer.fit(train_df)\n",
    "                synthesizer.save(f'{dataset}_{synthesizer_name}_{random_state}_{dp}.pkl')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discretize and save into csv\n",
    "\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "dps = [True, False]\n",
    "random_states = [42,52,62,72]\n",
    "#Generate discrete values\n",
    "for dataset in datasets:\n",
    "    for random_state in random_states:\n",
    "        for dp in dps:\n",
    "            filepath = f'./data/{dataset}/original_{dataset}_train_{random_state}.csv'\n",
    "            train_df = pd.read_csv(filepath)\n",
    "            print(filepath)\n",
    "            synthesizer = TVAESynthesizer.load(\n",
    "                filepath=f'{dataset}_tvae_{random_state}_{dp}.pkl'\n",
    "                )\n",
    "            print(synthesizer)\n",
    "            vae = synthesizer.get_vae()\n",
    "            discretized_space = vae.get_discretized()\n",
    "            discretized_space.to_csv(f'./discretized/{dataset}/{dataset}_{random_state}_{dp}.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "#Then perform t-way sampling using ACTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./t_way_vectors/travelcustomer/travelcustomer_42_True_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fabc03ef670>\n",
      "./t_way_vectors/travelcustomer/travelcustomer_42_False_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fab81a1c0d0>\n",
      "./t_way_vectors/adult/adult_42_True_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fab7a48f220>\n",
      "./t_way_vectors/adult/adult_42_False_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fab927d7d00>\n",
      "./t_way_vectors/creditdefault/creditdefault_42_True_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fab92bd6670>\n",
      "./t_way_vectors/creditdefault/creditdefault_42_False_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fab8063b2e0>\n",
      "./t_way_vectors/heloc/heloc_42_True_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fabc08487f0>\n",
      "./t_way_vectors/heloc/heloc_42_False_4_way.csv\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fabc0752c40>\n"
     ]
    }
   ],
   "source": [
    "#Generate sample and save into csv\n",
    "\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "dps = [True, False]\n",
    "random_states = [42]\n",
    "#Generate discrete values\n",
    "for dataset in datasets:\n",
    "    for random_state in random_states:\n",
    "        for dp in dps:\n",
    "            filepath = f'./t_way_vectors/{dataset}/{dataset}_{random_state}_{dp}_4_way.csv'\n",
    "            discretized_df = pd.read_csv(filepath)\n",
    "            print(filepath)\n",
    "            synthesizer = TVAESynthesizer.load(\n",
    "                filepath=f'{dataset}_tvae_{random_state}_{dp}.pkl'\n",
    "                )\n",
    "            print(synthesizer)\n",
    "            vae = synthesizer.get_vae()\n",
    "            t_way_samples = vae.sample_t_way_latent(discretized_df)\n",
    "            t_way_samples.to_csv(f'./data/{dataset}/ctvae_{dataset}_synth_{random_state}_{dp}_4_way.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CT-VAE Sample Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42]\n",
    "dps = [True, False]\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        for random_state in random_states:\n",
    "            # Read the training and test data\n",
    "            train_df = pd.read_csv(f'./data/{dataset}/ctvae_{dataset}_synth_{random_state}_{dp}_4_way.csv')\n",
    "            test_df = pd.read_csv(f'./data/{dataset}/original_{dataset}_test_{random_state}.csv')\n",
    "\n",
    "            # Splitting features and labels\n",
    "            ht = HyperTransformer()\n",
    "            X_train = train_df.iloc[:, :-1]\n",
    "            ht.detect_initial_config(data=X_train)\n",
    "            X_train = ht.fit_transform(X_train)\n",
    "            y_train = train_df.iloc[:, -1]\n",
    "\n",
    "            X_test = test_df.iloc[:, :-1]\n",
    "            ht.detect_initial_config(data=X_test)\n",
    "            X_test = ht.fit_transform(X_test)\n",
    "            y_test = test_df.iloc[:, -1]\n",
    "\n",
    "            # Initialize classifiers\n",
    "            lr = LogisticRegression(max_iter=1000)\n",
    "            dt = DecisionTreeClassifier()\n",
    "            rf = RandomForestClassifier()\n",
    "            svc = SVC(probability=True)  # Set probability to True for SVC\n",
    "\n",
    "            classifiers = [lr, dt, rf, svc]\n",
    "\n",
    "            for classifier in classifiers:\n",
    "                # Fit and predict using the classifier\n",
    "                classifier.fit(X_train, y_train)\n",
    "                y_pred = classifier.predict(X_test)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Calculate ROC AUC\n",
    "                if hasattr(classifier, \"predict_proba\"):\n",
    "                    roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "                else:\n",
    "                    roc_auc = None\n",
    "\n",
    "                # Calculate Precision-Recall AUC\n",
    "                if hasattr(classifier, \"predict_proba\"):\n",
    "                    precision, recall, _ = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                else:\n",
    "                    pr_auc = None\n",
    "\n",
    "                # Add results to the array\n",
    "                results.append([dataset, \"ct_vae\", random_state, str(classifier), accuracy, roc_auc, pr_auc])\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# Export results as a CSV file\n",
    "results_df.to_csv(\"ct_vae_tabular_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Evaluation\n",
    "\n",
    "#CT-VAE Sample Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42]\n",
    "dps = [True, False]\n",
    "results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        for random_state in random_states:\n",
    "            # Read the training and test data\n",
    "            train_df = pd.read_csv(f'./data/{dataset}/ctvae_{dataset}_synth_{random_state}_{dp}_4_way.csv')\n",
    "            test_df = pd.read_csv(f'./data/{dataset}/original_{dataset}_test_{random_state}.csv')\n",
    "\n",
    "            # Splitting features and labels\n",
    "            ht = HyperTransformer()\n",
    "            X_train = train_df.iloc[:, :-1]\n",
    "            ht.detect_initial_config(data=X_train)\n",
    "            X_train = ht.fit_transform(X_train)\n",
    "            y_train = train_df.iloc[:, -1]\n",
    "\n",
    "            X_test = test_df.iloc[:, :-1]\n",
    "            ht.detect_initial_config(data=X_test)\n",
    "            X_test = ht.fit_transform(X_test)\n",
    "            y_test = test_df.iloc[:, -1]\n",
    "\n",
    "            # Initialize classifiers\n",
    "            lr = LogisticRegression(max_iter=1000)\n",
    "            dt = DecisionTreeClassifier()\n",
    "            rf = RandomForestClassifier()\n",
    "            svc = SVC(probability=True)  # Set probability to True for SVC\n",
    "\n",
    "            classifiers = [lr, dt, rf, svc]\n",
    "\n",
    "            for classifier in classifiers:\n",
    "                # Fit and predict using the classifier\n",
    "                classifier.fit(X_train, y_train)\n",
    "                y_pred = classifier.predict(X_test)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Calculate ROC AUC\n",
    "                if hasattr(classifier, \"predict_proba\"):\n",
    "                    roc_auc = roc_auc_score(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "                else:\n",
    "                    roc_auc = None\n",
    "\n",
    "                # Calculate Precision-Recall AUC\n",
    "                if hasattr(classifier, \"predict_proba\"):\n",
    "                    precision, recall, _ = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                else:\n",
    "                    pr_auc = None\n",
    "\n",
    "                # Add results to the array\n",
    "                results.append([dataset, \"ct_vae\", random_state, str(classifier), accuracy, roc_auc, pr_auc])\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# Export results as a CSV file\n",
    "results_df.to_csv(\"ct_vae_tabular_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random DP-VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fc69948fdf0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fc68850b9a0>\n",
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fc69a4ed130>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sdv.single_table.ctgan.TVAESynthesizer object at 0x7fc67948b610>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnakhadka/opt/anaconda3/envs/sdv_dev_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "#Generate sample and save into csv\n",
    "\n",
    "from sdv.single_table import TVAESynthesizer\n",
    "\n",
    "\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "num_samples = [763, 34190, 21000, 7322]\n",
    "dps = [True]\n",
    "random_states = [42]\n",
    "#Generate discrete values\n",
    "for dataset in datasets:\n",
    "    i = 0\n",
    "    for random_state in random_states:\n",
    "        for dp in dps:\n",
    "            # filepath = f'./t_way_vectors/{dataset}/{dataset}_{random_state}_{dp}_4_way.csv'\n",
    "            # discretized_df = pd.read_csv(filepath)\n",
    "            # print(filepath)\n",
    "            synthesizer = TVAESynthesizer.load(\n",
    "                filepath=f'./saved_baseline_tabular_models/{dataset}_tvae_{random_state}_{dp}.pkl'\n",
    "                )\n",
    "            print(synthesizer)\n",
    "            vae = synthesizer.get_vae()\n",
    "            t_way_samples = vae.sample(num_samples[i] )\n",
    "            i = i+1\n",
    "            t_way_samples.to_csv(f'./data/{dataset}/dptvae_{dataset}_synth_{random_state}_{dp}_4_way.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluate with Statistical Evaluation\n",
    "#For CT-VAE with DP and non DP\n",
    "#Generate Samples and save\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, auc, precision_recall_curve\n",
    "from rdt import HyperTransformer\n",
    "from sdv.metrics.tabular import NumericalPrivacyMetric, CategoricalPrivacyMetric\n",
    "from sdmetrics.single_table import CategoricalKNN, NumericalMLP\n",
    "import warnings\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "warnings.filterwarnings('ignore')\n",
    "from rdt import HyperTransformer\n",
    "\n",
    "# Define the list of datasets, synthesizer names, and random states\n",
    "datasets = [\"travelcustomer\", \"adult\", \"creditdefault\", \"heloc\"]\n",
    "synthesizer_names = [\"tvae\", \"copula_gan\", \"ctgan\"]\n",
    "random_states = [42]\n",
    "results = []\n",
    "dps = [True, False]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dp in dps:\n",
    "        for random_state in random_states:\n",
    "            # Read the original and synth data\n",
    "            original_df = pd.read_csv(f'./{dataset}/original_{dataset}_train_{random_state}.csv')\n",
    "            synth_df = pd.read_csv(f'./{dataset}/ctvae_{dataset}_synth_42_{dp}_4_way.csv')\n",
    "            ht = HyperTransformer()\n",
    "            ht.detect_initial_config(data=original_df)\n",
    "            metadata = SingleTableMetadata()\n",
    "            metadata.detect_from_dataframe(data=original_df)\n",
    "            categorical_keys = []\n",
    "            continuous_keys = []\n",
    "            target_key = []\n",
    "            columns_metadata = metadata.columns\n",
    "            last_key = list(columns_metadata.keys())[-1]\n",
    "            target_key.append(last_key)\n",
    "\n",
    "            for key, value in columns_metadata.items():\n",
    "                if key == last_key:\n",
    "                    continue  # Skip the last key since it's already in the \"other\" category\n",
    "                if value[\"sdtype\"] == \"categorical\":\n",
    "                    categorical_keys.append(key)\n",
    "                elif value[\"sdtype\"] == \"numerical\":\n",
    "                    continuous_keys.append(key)\n",
    "            # print(categorical_keys, continuous_keys, target_key)\n",
    "            # continue\n",
    "            num_privacy = NumericalMLP.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=continuous_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            try:\n",
    "                cat_privacy = CategoricalKNN.compute(\n",
    "                    real_data = original_df, \n",
    "                    synthetic_data= synth_df,\n",
    "                    key_fields=categorical_keys,\n",
    "                    sensitive_fields= target_key)\n",
    "            except Exception as e:\n",
    "                cat_privacy = num_privacy\n",
    "            privacy_score = (num_privacy + cat_privacy)/2\n",
    "\n",
    "\n",
    "            print(f'{dataset}_{dp}_{random_state}: Numerical Privacy: {num_privacy}, Categorical Privacy: {cat_privacy}, Privacy: {privacy_score}')\n",
    "            \n",
    "\n",
    "        \n",
    "# Convert results to a DataFrame\n",
    "# results_df = pd.DataFrame(results, columns=[\"Dataset\", \"Synthesizer\", \"Random_State\", \"Classifier\", \"Accuracy\", \"ROC_AUC\", \"PR_AUC\"])\n",
    "\n",
    "# # Export results as a CSV file\n",
    "# results_df.to_csv(\"results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdv_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
